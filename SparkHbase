val connection = ConnectionFactory.createConnection(HBaseConfiguration.create())
val tableName = TableName.valueOf("namespace:table")
val table = connection.getTable(tableName)
val scan = new Scan().setLimit(10)
val scanner = table.getScanner(scan)

scanner.forEach { result =>
  val cfMap = result.getNoVersionMap
  cfMap.forEach { (cf, qMap) =>
    val cfStr = Bytes.toString(cf)
    qMap.forEach { (q, _) =>
      val qualifier = Bytes.toString(q)
      println(s"$cfStr:$qualifier")
    }
  }
}

val conf = HBaseConfiguration.create()
conf.set("hbase.zookeeper.quorum", "localhost")
conf.set(TableInputFormat.INPUT_TABLE, "namespace:your_table")

val hBaseRDD = spark.sparkContext.newAPIHadoopRDD(
  conf,
  classOf[TableInputFormat],
  classOf[ImmutableBytesWritable],
  classOf[Result]
)

val rowRDD = hBaseRDD.map { case (_, result) =>
  val rowKey = Bytes.toString(result.getRow)
  val name = Bytes.toString(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("name")))
  val city = Bytes.toString(result.getValue(Bytes.toBytes("location"), Bytes.toBytes("city")))

  (rowKey, name, city)
}

val df = rowRDD.toDF("id", "name", "city")
df.show()

import org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat
import org.apache.hadoop.hbase.client.Result
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.mapreduce.Job

val conf = HBaseConfiguration.create()
conf.set("hbase.rootdir", "hdfs://your-hbase-rootdir") // very important!
conf.set("hbase.zookeeper.quorum", "zk1,zk2,zk3")

conf.set(TableSnapshotInputFormat.SNAPSHOT_NAME_KEY, "snapshot_name")
conf.set(TableSnapshotInputFormat.SNAPSHOT_DIR_KEY, "/path/to/snapshot_export")

val job = Job.getInstance(conf)
TableSnapshotInputFormat.setInput(job, "snapshot_name", new Path("/path/to/snapshot_export"))

val snapshotRDD = spark.sparkContext.newAPIHadoopRDD(
  job.getConfiguration,
  classOf[TableSnapshotInputFormat],
  classOf[ImmutableBytesWritable],
  classOf[Result]
)

✅ 1. Connect to HBase and Get Table Schema
val conf = HBaseConfiguration.create()
val connection = ConnectionFactory.createConnection(conf)
val admin = connection.getAdmin

val tableName = TableName.valueOf("namespace:your_table_name")
val descriptor = admin.getDescriptor(tableName)

val columnFamilies = descriptor.getColumnFamilies.map(_.getNameAsString)

✅ 2. (Optional) Define known qualifiers for each column family
Since HBase doesn't store qualifiers in schema, you’ll need to scan a few rows to extract the qualifiers dynamically:

val table = connection.getTable(tableName)
val scan = new Scan().setLimit(10) // scan first 10 rows
val scanner = table.getScanner(scan)

val colMap = scala.collection.mutable.Map[String, scala.collection.mutable.Set[String]]()

scanner.forEach { result =>
  val cfMap = result.getNoVersionMap
  cfMap.forEach { (cf, qMap) =>
    val cfStr = Bytes.toString(cf)
    qMap.forEach { (q, _) =>
      val qStr = Bytes.toString(q)
      val set = colMap.getOrElseUpdate(cfStr, scala.collection.mutable.Set())
      set += qStr
    }
  }
}
scanner.close()
Now colMap will have:

Map(
  "info" -> Set("name", "age"),
  "location" -> Set("city")
)

✅ 3. Build hbase.columns.mapping String
Assuming your row key column is "id":

val rowKey = "id"

val mappingStr = 
  Seq(s"key=$rowKey") ++ 
  colMap.toSeq.flatMap { case (cf, qualifiers) =>
    qualifiers.toSeq.map(q => s"$cf:$q")
  }
val finalMapping = mappingStr.mkString(",")
✅ 4. Example Output
finalMapping
// Output: "key=id,info:name,info:age,location:city"
You can now pass this to:

.option("hbase.columns.mapping", finalMapping)
✅ All Together in a Function?

Would you like a reusable function like:

def generateHBaseColumnMapping(tableName: String, rowKeyCol: String): String









